# 🧠 DeepLearning.AI Data Engineering Professional Certificate

Welcome to the DeepLearning.AI Data Engineering Professional Certificate project repository! This comprehensive program equips you with the skills and knowledge to navigate the exciting world of data engineering.

Through hands-on learning experiences, you'll gain a solid foundation in the data engineering lifecycle, data architecture principles, and essential tools and technologies used on the AWS cloud.

## 📚 What You'll Learn

- **📈 Master the data engineering lifecycle**: Develop a comprehensive understanding of the five key stages – data generation, ingestion, storage, transformation, and serving – and their significance in creating business value with data.
- **🧩 Embrace a data-centric mindset**: Learn a practical framework for approaching data engineering projects, allowing you to effectively translate stakeholder needs into actionable systems.
- **🏗️ Build robust data architectures**: Gain the knowledge to design and implement data systems on the AWS cloud, adhering to sound data architecture principles for optimal performance and scalability.
- **🛠️ Hone essential data engineering skills**: Master the five core stages of the data engineering lifecycle, mastering data management, data transformation, data warehousing, data modeling, and data orchestration techniques.
- **🛠️ Become proficient in industry-standard tools**: Equip yourself with the expertise to leverage tools like Spark and PySpark for data processing, AWS services for data storage and management, and infrastructure as code (IaC) principles.
- **🗄️ Explore advanced data storage solutions**: Gain insights into designing data storage architectures for various use cases, exploring technologies like data warehouses, data lakes, and data lakehouses.
- **🔍 Enhance data querying capabilities**: Dive deeper into advanced SQL concepts and discover techniques to improve query performance and extract maximum value from your data systems.

## 🛠️ Skills Covered

- Data Modeling
- Feature Engineering
- Data Transformation
- Data Management
- DataOps
- Data Warehousing
- Data Orchestration
- Spark & PySpark
- Advanced SQL
- AWS Cloud Fundamentals
- Data Storage Architectures
- Data Ingestion

## 📅 Program Outline

This program is structured as 4 courses.

### 📘 Course 1 - Introduction to Data Engineering

**This course consists of 4 weeks of content and covers these main learning objectives:**

- Identify key upstream and downstream collaborators and stakeholders for data engineers
- Articulate a mental framework for building data engineering solutions
- Identify some of the necessary considerations for requirements gathering at the start of a new project
- Describe the structure of the data engineering lifecycle and its undercurrents, and how to think about data engineering problems through this lens
- Identify some of the key technologies that can be employed in different stages of the data engineering lifecycle
- Evaluate technologies and tools against the context of requirements and good data architecture
- Design a data architecture on AWS based on stakeholder requirements
- Implement a batch and streaming pipeline on AWS to support a product recommendation system


### 📥 Course 2 - Source Systems, Data Ingestion, and Pipelines

**This course consists of 4 weeks of content and covers these main learning objectives:**

- Identify different data formats and determine appropriate source systems for generating each type of data
- Explain at a high level how data is generated, stored, and retrieved in various source systems, including relational databases, NoSQL databases, object storage, and streaming systems
- Explain the basics of cloud networking
- Troubleshoot database connection errors
- Explain the difference between batch and streaming ingestions and identify uses cases for each pattern
- Differentiate between the two batch ingestion patterns: Extract-Transform-Load (ETL) and Extract-Load-Transform (ELT)
- Create a script to ingest data from a REST API
- Describe the basic components of an event-streaming platform
- Interact with an event streaming platform as a source system and as an ingestion tool
- Use Terraform to provision AWS resources for your data pipeline
- Identify tools for monitoring your data systems and data quality
- Identify and monitor relevant data quality metrics
- Explain how orchestration can be applied to a data pipeline, and list its benefits
- Build data pipelines with DAGs in Airflow using features such as Taskflow API, operators, XCom variables, etc.

### 🗄️ Course 3 - Data Storage and Queries

**This course consists of 3 weeks of content and covers these main learning objectives:**

- Explain how data is physically stored on disk and in memory
- Compare how data is stored and queried in object, block, and file storage systems
- Explain how data is stored in row-oriented vs column-oriented databases
- Explain how graph and vector databases store and retrieve data
- Explain the key architectural features of data warehouses, data lakes, and data lakehouses
- Implement a data lake using AWS Glue
- Implement a data lakehouse with a medallion-like architecture using Lake Formation and Iceberg
- Explain the stages of the life of a query
- Implement advanced SQL queries
- Explain the role of an index and its impact on query performance
- Summarize approaches for processing aggregate and join queries
- Compare the execution times of aggregate queries between row and columnar storage
- List some strategies for enhancing query performance
- Aggregate and join streaming data


### 🔄 Course 4 - Data Modeling, Transformation, and Serving

**This course consists of 4 weeks of content and covers these main learning objectives:**

- Define data modeling and its role in reflecting business logic
- Apply the normalization stages to a denormalized table
- Describe the fact and dimension tables of a star schema and transform data in third normal form to a star schema
- Describe the data warehouse modeling approaches such as Inmon, Kimball, Data Vault, and One Big Table
- Use feature engineering to convert a dataset into a tabular form that’s expected by classical machine learning algorithms
- Preprocess and vectorize textual data
- List techniques for processing and augmenting image data
- Compare an in-memory processing framework like Spark, and a disk-based processing framework like Hadoop
- Describe the technical considerations for choosing a distributed processing framework, such as Spark, vs a non-distributed framework like Pandas dataframes
- Describe the technical considerations for using Spark SQL vs Spark DataFrames when transforming data using PySpark
- Describe how streaming transformation works with a near-real time processing engine such as Spark Structured Streaming
- Identify different ways of serving data for analytics and machine learning use cases
- Describe the purpose of a semantic layer that sits on top of the data model
- Create views and materialized views
- Describe the benefits and drawbacks of serving data using views and materialized views


## 🚀 Embark on Your Data Engineering Journey!

This DeepLearning.AI Data Engineering Professional Certificate program empowers you to become a valuable asset in the data-driven world. Enroll today and take your first step towards mastering the art of data engineering!
